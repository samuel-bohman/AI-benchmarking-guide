{
    "GEMMCublasLt": {
        "inputs": {
            "m": {
                "start": 16,
                "end": 4096,
                "interval": 16
            },
            "n": {
                "start": 16,
                "end": 4096,
                "interval": 16
            },
            "k": {
                "start": 16,
                "end": 4096,
                "interval": 16
            },
            "duration": 120,
            "datatype": "fp8e4m3"
        }
    },
    
    "GEMMHipBLAS": {
        "inputs": {
            "m": {
                "start": 16,
                "end": 4096,
                "interval": 16
            },
            "n": {
                "start": 16,
                "end": 4096,
                "interval": 16
            },
            "k": {
                "start": 16,
                "end": 4096,
                "interval": 16
            },
            "duration": 120,
            "datatype": "fp8e4m3"
        }
    },

    "NCCLBandwidth": {
        "inputs": {
            "start": "8",
            "end": "8G",
            "num_gpus": 8
        }
    },

    "HBMBandwidth": {
        "inputs": {
            "interval": 10,
            "num_runs": 5
        }
    },

    "RCCLBandwidth": {
        "inputs": {
            "start": "8",
            "end": "8G",
            "num_gpus": 8
        }
    },

    "NVBandwidth": {
        "inputs": {
            "num_runs": 1,
            "interval": 5
        }
    },

    "TransferBench": {
        "inputs": {
            "num_runs": 1,
            "interval": 5
        }
    },
    "LLMBenchmark": {
        "models": {
            "meta-llama/Llama-3.1-8B":{
                "use_model": true,
                "type": "nvidia",
                "input_sizes": [128, 128, 500, 2048, 1024, 128],
                "output_sizes": [128, 2048, 2000, 2048, 1024, 1024],
                "tp_size": 1,
                "num_requests": 1000,
                "precision": "FP8"
            },

            "meta-llama/Llama-3.1-70B":{
                "use_model": false,
                "type": "nvidia",
                "input_sizes": [128, 128, 500, 2048, 1024, 128],
                "output_sizes": [128, 2048, 2000, 2048, 1024, 1024],
                "tp_size": 8,
                "num_requests": 1000,
                "precision": "FP8"
            },

            "meta-llama/Llama-3.1-405B":{
                "use_model": false,
                "type": "nvidia",
                "input_sizes": [128, 128, 500, 2048, 1024, 128],
                "output_sizes": [128, 2048, 2000, 2048, 1024, 1024],
                "tp_size": 8,
                "num_requests": 1000,
                "precision": "FP8"
            },

            "Meta-Llama-3.1-8B-Instruct-FP8-KV":{
                "use_model": true,
                "type": "amd",
                "max_num_seqs": [3000],
                "input_length":[128, 128, 500, 2048, 1024, 128],
                "output_length": [128, 2048, 2000, 2048, 1024, 1024],
                "tp_sizes": [1],
                "num_requests": [1000]
            },

            "Meta-Llama-3.1-70B-Instruct-FP8-KV":{
                "use_model": false,
                "type": "amd",
                "max_num_seqs": [3000],
                "input_length":[128, 128, 500, 2048, 1024, 128],
                "output_length": [128, 2048, 2000, 2048, 1024, 1024],
                "tp_sizes": [8],
                "num_requests": [1000]
            },

            "Meta-Llama-3.1-405B-Instruct-FP8-KV":{
                "use_model": false,
                "type": "amd",
                "max_num_seqs": [3000],
                "input_length":[128, 128, 500, 2048, 1024, 128],
                "output_length": [128, 2048, 2000, 2048, 1024, 1024],
                "tp_sizes": [8],
                "num_requests": [1000]
            }
        }
    }
}
